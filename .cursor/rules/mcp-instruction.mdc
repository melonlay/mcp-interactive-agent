---
description: 
globs: 
alwaysApply: true
---
# Interactive GUI MCP Server Interaction Protocol

This document outlines the principles, workflows, tools, and safety guidelines for the AI assistant when interacting with the Interactive GUI MCP (Model Context Protocol) Server.

## I. Core Interaction Principles with MCP Server

These are fundamental rules governing the AI assistant's behavior when using the MCP server.

1.  **Autonomous Progression (自主推進)**:
    *   Once task objectives are clear, autonomously advance the workflow, especially following the "Advanced Workflow" (see Section II).
    *   After a tool call (e.g., `capture_screen`) completes successfully, if the workflow defines a subsequent automated step dependent on this result (e.g., `generate_text_from_google` analyzing the screenshot), **immediately execute** that next step without waiting for explicit user instruction.
    *   **Avoid describing intent without execution**: When the AI decides a tool call is necessary, it must *initiate* the call, not merely inform the user it *will* make the call.

2.  **Minimize Unnecessary Pauses (最小化不必要停頓)**:
    *   Strive to complete as much of the task sequence as possible in one go, reducing unnecessary back-and-forth confirmations with the user.
    *   Only pause and request user input if:
        *   The process encounters ambiguity requiring user decision.
        *   An error occurs that needs user diagnosis or resolution.
        *   The task explicitly requires manual verification at a specific stage.
        *   The user's complete request has been fulfilled.

3.  **Immediate Tool Invocation (工具呼叫的即時性)**:
    *   When internal logic dictates the use of an MCP tool, immediately construct and execute the corresponding tool call. Do not announce intent and wait for confirmation; execute and report based on the outcome.

4.  **Internal State and Flow Tracking (內部狀態與流程追蹤)**:
    *   Maintain a clear internal understanding of the current workflow state, ensuring each step executes as planned. If a planned tool call is not initiated, treat this as an internal error and attempt to rectify or report.

5.  **Mandatory Execution of Intent to Action (意圖即行動的強制執行)**:
    *   Once the AI's internal logic decides to call an MCP tool, that tool call **must be the next and non-skippable execution step**.
    *   **No "Declare and Delay"**: After deciding on and preparing a tool call (including parameters), do not generate intermediate conversational replies or perform other non-tool-call processing. The tool call must be initiated immediately.
    *   **Internal Execution Commitment**: Forming the intent for a tool call is an irrevocable commitment to immediate execution.
    *   **Pre-Response Verification (Conceptual)**: Before sending any response to the user, internally verify: if the most recent internal decision was to initiate a tool call, has it been executed? If not, execute it first, then form the response based on its result or error.

6.  **Tool Call as Step Core (工具呼叫視為步驟核心)**:
    *   In multi-step tasks, if a step's core is a tool call, the step's completion **requires the tool's successful execution and the return of its results**. Merely describing the intent to call a tool does not constitute step completion.

7.  **Strict Adherence to Sequential Flow (嚴格執行連續流程)**:
    *   When following defined workflows (like the "Advanced Workflow"), **ensure continuity between steps**. E.g., after `capture_screen` success, if the next step is image analysis, seamlessly transition to the analysis tool call.
    *   Any non-error or non-decision-point pause is a violation of this principle.

8.  **Recalibrate "Task Completion" Judgement (重新校準「工作結束」的判斷)**:
    *   Judge sub-task or full-task completion based on achieving substantive goals (e.g., successful operation execution, information acquisition), not just verbal description of the task.

9.  **Absolute Accuracy of Tool Names (工具名稱的絕對準確性)**:
    *   Before constructing or attempting any MCP tool call, **must** verify and use the tool's **exact name** from the provided API definitions. No variations (hyphens vs. underscores, case errors) are permitted.
    *   If an expected tool name is not found in the API definitions, report this specific issue as a configuration or understanding error, rather than attempting to call a non-existent or misspelled tool.

10. **Precision in Parameter Construction and Pre-flight Validation (參數建構的精確性與預檢驗)**:
    *   Strictly adhere to the parameter type requirements (e.g., `str`, `int`, `bool`, `list[str]`, specific `dataclass`) as defined in the tool's API.
    *   For `dataclass` parameters, provide a Python `dict` with keys matching field names, or use the `dataclass` constructor if context allows (not JSON strings).
    *   **Pre-flight Validation**: Internally verify:
        *   a. Tool name matches API definition.
        *   b. All parameters match declared structure and core types.
    *   If validation fails, report the specific error and attempt correction, not blind execution.

## II. GUI Automation: Advanced Workflow & Strategy

This section details the iterative process for achieving precise GUI automation.

### A. Overall Workflow

1.  **Initial State Capture (`capture_screen`)**:
    *   Obtain a full screenshot and its dimensions as a baseline for analysis.
    *   *Tool*: `mcp_interactive-gui-mcp_capture_screen`

2.  **Target Element Identification & Iterative Validation**:
    This is primarily AI-driven, combining `mcp_interactive-gui-mcp_generate_text_from_google` for broad understanding and `mcp_interactive-gui-mcp_get_subarea_description` for focused analysis. The goal is to determine `focused_bounds` (global x, y, width, height) for the target UI element.

    *   **a. Preliminary Full-Screen Description (using `mcp_interactive-gui-mcp_generate_text_from_google`)**:
        *   Analyze the complete screenshot to get a **textual description** of the target element (e.g., "main search input field," "submit button") including visual features and its general relative position on the screen (e.g., "top-center of page," "below the user profile icon").
        *   **Critical Principle**: At this stage, **do not** request precise coordinates from `mcp_interactive-gui-mcp_generate_text_from_google`. The aim is to gather descriptive clues.

    *   **b. AI Assistant: Estimate Initial `exploratory_bounds`**:
        *   Based on the full-screen description, the AI assistant **autonomously estimates** initial, somewhat generous `exploratory_bounds` that should contain the target element.
        *   If the initial description suggests the target is within a large, known section of the UI (e.g., "in the main content panel"), the `exploratory_bounds` should first target this larger section.

    *   **c. AI Assistant: Iterative Exploration & Localization (using `mcp_interactive-gui-mcp_get_subarea_description`)**:
        *   Define the current `exploratory_bounds`.
        *   Prompt `mcp_interactive-gui-mcp_get_subarea_description` (textual description, no coordinate requests): "In this sub-image (defined by my `exploratory_bounds`), describe the visual content. Based on features like [quote features from step 2a], is the target element [target name] visible? If so, describe its **approximate relative position *within this sub-image*** (e.g., 'top-left corner of this view', 'spanning the center', 'in the bottom-right quadrant'). **Do not provide numerical coordinates or percentages.**"
        *   Analyze the returned description:
            *   **Target Found**: If the target is visible and its position plausible, proceed to refine these `exploratory_bounds` into a smaller "candidate area". For example, if found in the "bottom-right quadrant" of the current `exploratory_bounds`, calculate the bounds for that quadrant and use it as the new, smaller `exploratory_bounds`. Repeat step 2c with this smaller area, asking for more specific relative positioning (e.g., "does it fill this new area?", "is it centered?"). This iterative refinement can be done 1-2 times to significantly narrow down the location.
            *   **Target Not Visible/Implausible**: If the target is not visible or its described position is implausible, the AI assistant **must autonomously decide** how to adjust `exploratory_bounds` (e.g., pan to an adjacent area based on initial full-screen description, zoom out if bounds were too tight, or apply systematic scrolling if element might be off-screen) and repeat step 2c.
            *   This loop is AI-driven until the target is localized within a reasonably small and confirmed `exploratory_bounds`.

    *   **d. AI Assistant: Define and Confirm `focused_bounds`**:
        *   **Estimate `focused_bounds`**: Based on the successfully refined `exploratory_bounds` (now a "candidate area") and the target's most recent relative position description (e.g., "centered in the lower half of this candidate area"), the AI **autonomously estimates** tighter `focused_bounds` intended to closely encompass the target element. This involves converting the relative description (e.g., "lower half") into concrete coordinate adjustments from the "candidate area" bounds.
        *   **Initial `focused_bounds` Check (using `mcp_interactive-gui-mcp_get_subarea_description`)**: Pass the estimated `focused_bounds` and ask: "Does this sub-image (defined by my `focused_bounds`) show a clear view of the [target element name from 2a]? Is the target element fully contained, or does it appear to be cut off on any side? Describe key visual features."
        *   **Handle裁切 (Cut-off) Feedback**: If the description indicates the element is cut off (e.g., "text is cut off on the left", "only right half of button visible"), adjust the `focused_bounds` accordingly (e.g., decrease X and increase width if cut off on left; increase Y if cut off at top). Re-check with `mcp_interactive-gui-mcp_get_subarea_description` after adjustment.
        *   **Iterative Purity Check & Refinement (see also III.3 Bounding Box Purity Principle)**:
            *   Once the element is fully contained, pass the `focused_bounds` to `mcp_interactive-gui-mcp_get_subarea_description` and ask: "Does this sub-image (defined by my `focused_bounds`) show a clear, close-up view of the [target element name from 2a]? **Critically, does it *only* contain this one primary target element, or are other distinct clickable elements or significant distracting text/graphics also visible?** Describe any non-target elements and their position relative to the target (e.g., 'another button to the right', 'text directly above')."
            *   **Refine based on Impurities**: If non-target elements are reported (e.g., "a small icon is to the right of the button"), refine `focused_bounds` to exclude them. For instance, if an icon is on the right, reduce the `width` of `focused_bounds` by an estimated amount for that icon and any spacing. If text is directly above, increase `y` and decrease `height`.
            *   Repeat this purity check and refinement 1-2 times.
        *   **Final Decision**:
            *   If the description confirms a clear view of the target, fully contained, and reasonably pure (see III.3 for acceptable purity levels), these `focused_bounds` are considered validated.
            *   If significant impurities persist or the element cannot be isolated after 2-3 refinement attempts, the AI may need to log this and decide if the current (less pure) `focused_bounds` are still acceptable for interaction based on risk (see III.3).

    *   **e. AI Assistant: Obtain Final Coordinates**:
        *   The target element's final global coordinates are the AI-defined and validated `focused_bounds` (x, y, width, height).

3.  **Calculate Interaction Point & Execute Operation (`mcp_interactive-gui-mcp_control_input`)**:
    *   Based on the final `focused_bounds`, calculate the precise interaction point.
        *   Default: Center point (`focused_bounds.x + focused_bounds.width / 2`, `focused_bounds.y + focused_bounds.height / 2`).
        *   Refinement (See Section II.B): Adjust from center if `mcp_interactive-gui-mcp_get_subarea_description` provided more specific relative location details *within* the `focused_bounds` for the exact clickable sub-feature.
    *   Execute the operation (e.g., click, type) using `mcp_interactive-gui-mcp_control_input`. Include short waits if necessary for UI to react.

4.  **Post-Operation State Validation (Crucial Iterative Step)**:
    *   **Re-capture Screen**: `mcp_interactive-gui-mcp_capture_screen` to get the new UI state.
    *   **Validate Changes**: Use `mcp_interactive-gui-mcp_get_subarea_description` (or `mcp_interactive-gui-mcp_generate_text_from_google` for broader changes) on the new screenshot, targeting the previously interacted element's `focused_bounds` (or a relevant area of effect).
    *   Prompt to verify expected changes: "Is this area now [expected state, e.g., 'an active input field', 'showing the text I typed']? Did surrounding elements change as expected (e.g., 'did a dropdown menu appear below')?"
    *   Compare AI feedback to the expected state.

5.  **Process Validation Outcome (Success/Failure)**:
    *   **Success**: Proceed to the next operation.
    *   **Failure**:
        *   Log the reason (e.g., click ineffective, text not entered).
        *   **Iterate/Correct**: Return to step 2 to re-locate the same element (it might have shifted or changed state) or an alternative if the initial attempt was flawed. Adjust descriptive prompts or bounding box estimation strategies.
        *   **Core Principle**: Do not proceed with subsequent operations if a key preceding operation has not been successfully validated.

6.  **Continue Subsequent Operations with Validation**:
    *   For every critical subsequent GUI operation, follow it with a validation loop (like steps 4 & 5).

7.  **Final Result Analysis**:
    *   After all interactions, capture the final screen state. Use `mcp_interactive-gui-mcp_generate_text_from_google` (overall description) or `mcp_interactive-gui-mcp_get_subarea_description` (specific areas) to analyze if the overall task goals were met.

### B. Key Techniques for Visual Analysis & Interaction

1.  **Handling Vision Tool Output Variability/Inconsistency**:
    *   Vision models (like the one backing `mcp_interactive-gui-mcp_get_subarea_description`) can be probabilistic. If consecutive calls on *identical* bounds yield conflicting critical information (e.g., element visibility, title text):
        *   **Prioritize Affirmative Detail**: If one response is more detailed and affirmative (e.g., quotes specific text found), it may be more reliable than a vague negative.
        *   **Perturb and Re-query**: Consider slightly perturbing the bounds (e.g., shift by a few pixels, slightly enlarge/shrink by <5%) and call `mcp_interactive-gui-mcp_get_subarea_description` again. A consistent result across minor variations increases confidence.
        *   **Cross-Reference**: Compare sub-area findings with the initial full-screen `mcp_interactive-gui-mcp_generate_text_from_google` description for consistency.
    *   If critical ambiguity persists after 2-3 attempts on a crucial element, and autonomous resolution fails, this may be a rare exception point to flag internal uncertainty before proceeding with a high-risk action.

2.  **Refining Interaction Points within `focused_bounds`**:
    *   The geometric center of `focused_bounds` is a starting point for clicks.
    *   However, if `mcp_interactive-gui-mcp_get_subarea_description` for the `focused_bounds` provides specific textual cues about the *internal relative position* of the exact clickable feature (e.g., "The submit button is in the bottom-right of this view," "The link text is on the left side"), **adjust the click coordinates** from the center towards that described relative position. For example:
        *   Bottom-right: `click_x = bounds.x + bounds.width * 0.75`, `click_y = bounds.y + bounds.height * 0.75`
        *   Left-center: `click_x = bounds.x + bounds.width * 0.25`, `click_y = bounds.y + bounds.height * 0.5`
    *   This is crucial when the `focused_bounds` identify a larger component, but the click target is a smaller part of it.

3.  **Systematic Scrolling for Off-Screen Elements**:
    *   If an initial screen analysis does not locate the target element:
        *   Employ a systematic scrolling strategy (e.g., scroll down by ~80% of the screen height using `mcp_interactive-gui-mcp_control_input`).
        *   After scrolling, `mcp_interactive-gui-mcp_capture_screen` and restart the analysis from Step 2a on the new view.
        *   Avoid very small, fixed scroll amounts as they are inefficient.

## III. Safety, Validation, and Operational Integrity

These principles ensure robust, safe, and verifiable GUI automation.

1.  **Absolute Safety Principle & Certainty**:
    *   Before executing any interaction (`click`, `type`, `drag`), aim for the highest possible certainty regarding the target element's identity and location.
    *   Certainty is built through multi-step textual and visual cross-validation, not reliance on raw numerical coordinates alone.

2.  **Multi-Factor Validation & Pre-Operation Checklist**:
    *   Before critical operations, internally verify:
        *   ✅ Target element's textual description matches.
        *   ✅ Element's observed relative position on screen is consistent with expectations.
        *   ✅ Element's specific visual features (icons, text, color) are confirmed via `mcp_interactive-gui-mcp_get_subarea_description` within `focused_bounds`.
        *   ✅ `focused_bounds` are "pure" or "acceptably pure" (see Bounding Box Purity).

3.  **Bounding Box Purity Principle (邊界框純淨性原則)**:
    *   The `focused_bounds` used for an interaction (especially clicks) must **ideally exclusively and precisely contain the target interactive element**.
    *   It must not include other unrelated interactive elements, significant background portions, or other UI components that could lead to an incorrect click.
    *   **Verification**: Use `mcp_interactive-gui-mcp_get_subarea_description` on the `focused_bounds` with a prompt like: "Does this image exclusively show the [target element, e.g., 'Login button'] and no other clickable elements? Is it a tight fit around it? Describe any other elements present."
    *   **Refinement**: If non-target elements are present or the bounds are too loose, shrink/adjust the `focused_bounds` (e.g., by 5-10% iteratively from edges based on description of where non-target elements are) while keeping the target centered or aligned, and re-verify until pure. Handle "cut-off" element feedback by expanding the necessary side of the bounds.
    *   **Acceptable Purity**: If perfect purity cannot be achieved after 2-3 refinement attempts, evaluate if the current `focused_bounds` are "acceptably pure". This means:
        *   The target element is clearly and fully visible.
        *   Any non-target elements are minor, visually distinct from the target, and unlikely to interfere with the intended interaction point on the target.
        *   The risk of mis-click is deemed low.
        *   If these conditions are met, the AI can proceed with interaction using these "acceptably pure" bounds, but should log the state of purity.
    *   **Failure**: If purity (or acceptable purity) cannot be achieved, do not interact and flag the issue, potentially returning to an earlier stage of localization or asking for user guidance if stuck.

4.  **Coordinate System Awareness & Mapping Continuity (座標映射與連續性原則)**:
    *   All AI-estimated `exploratory_bounds` and `focused_bounds` are global screen coordinates.
    *   When `mcp_interactive-gui-mcp_get_subarea_description` describes an element's position *within* given bounds, this is relative. The AI must correctly map these relative descriptions back to global screen coordinate adjustments if necessary.
    *   Adjustments to bounds should be logical and based on textual descriptions from `mcp_interactive-gui-mcp_get_subarea_description`. Avoid large, unjustified jumps in coordinates between iterations. Track the evolution of bounds.

5.  **Sub-Area Validation Protocol (子區域驗證流程)**:
    *   Any GUI interaction must be preceded by at least two layers of visual validation:
        *   Layer 1: Full-screen context via `mcp_interactive-gui-mcp_generate_text_from_google`.
        *   Layer 2: Precise target validation within AI-defined `focused_bounds` using `mcp_interactive-gui-mcp_get_subarea_description`.
    *   Validation criteria: 100% match of visual features (textually described), no non-target elements or ambiguity within final `focused_bounds` for interaction.
    *   Failure handling: If sub-area validation fails after 2-3 refinement iterations, do not operate and report.

6.  **Risk Mitigation & Minimal Intrusion**:
    *   Operations should be predictable and minimally invasive.
    *   Avoid actions if high uncertainty remains after the validation workflow.

7.  **Detailed Logging and Transparency (日誌與追蹤)**:
    *   Internally (conceptually) log:
        *   Each estimation of `exploratory_bounds` and `focused_bounds`.
        *   Key inputs and outputs for `mcp_interactive-gui-mcp_generate_text_from_google` and `mcp_interactive-gui-mcp_get_subarea_description` calls.
        *   Reasons for bounding box adjustments.
        *   Confirmation/failure of validation steps.
    *   Be transparent about uncertainty if it significantly impacts decision-making.

## IV. Tool Reference

(Ensure tool names and parameters are used *exactly* as defined below.)

### 1. `mcp_interactive-gui-mcp_say_greeting`
*   **Purpose**: Simple greeting tool for testing basic MCP server connectivity.
*   **Args**: `message` (str): Greeting message. Returns "hello" if message is "hello", else "hi".
*   **Returns**: (str) Greeting reply.

### 2. `mcp_interactive-gui-mcp_capture_screen`
*   **Purpose**: Captures the entire main screen, saves to a temp directory, returns path and dimensions.
*   **Args**: `current_working_dir` (str): Workspace path. Screenshot saved in `current_working_dir/tmp/`.
*   **Returns** (object/dict):
    *   `file_path` (str): Full path to the saved screenshot.
    *   `width` (int): Screenshot width (pixels).
    *   `height` (int): Screenshot height (pixels).

### 3. `mcp_interactive-gui-mcp_generate_text_from_google`
*   **Purpose**: Uses Google Generative AI (Gemini) for image description based on a prompt and image file.
*   **Args**:
    *   `prompt` (str): Text prompt for the AI model.
    *   `image` (`McpInteractive-gui-mcpGenerateTextFromGoogleImage`): Contains `file_path` (str) to the local image file.
        *   e.g., `image=McpInteractive-gui-mcpGenerateTextFromGoogleImage(file_path='your/path/image.png')`
*   **Returns**: (str) AI-generated text description.

### 4. `mcp_interactive-gui-mcp_get_subarea_description`
*   **Purpose**: Crops a sub-area from an image, then uses Google Generative AI to describe this sub-area.
*   **Args**:
    *   `image_path` (str): Full path to the original image file.
    *   `bounds` (`McpInteractive-gui-mcpGetSubareaDescriptionBounds`): Defines sub-area to crop.
        *   e.g., `bounds=McpInteractive-gui-mcpGetSubareaDescriptionBounds(x=10, y=10, width=100, height=100)`
    *   `prompt` (str): Text prompt for describing the cropped sub-area.
*   **Returns**: (str) AI-generated text description of the sub-area.

### 5. `mcp_interactive-gui-mcp_control_input`
*   **Purpose**: Executes a sequence of keyboard and mouse control commands.
*   **Args**:
    *   `commands` (List[`McpInteractive-gui-mcpControlInputCommands`]): List of command objects, executed sequentially.
        *   **Command Structure (`McpInteractive-gui-mcpControlInputCommands`)**:
            *   `action` (str): Required. E.g., `"click"`, `"type"`, `"press"`, `"hotkey"`, `"move_to"`, `"drag_to"`, `"scroll"`, `"wait"`.
            *   `x` (int, optional): Target X for mouse actions.
            *   `y` (int, optional): Target Y for mouse actions.
            *   `text` (str, optional): Text for `type`.
            *   `keys` (str or List[str], optional): Key(s) for `press`, `hotkey`.
            *   `duration` (float, optional, default 0.2s): Mouse movement duration.
            *   `button` (str, optional, default "left"): Mouse button ("left", "middle", "right").
            *   `clicks` (int, optional, default 1): Number of clicks.
            *   `scroll_amount` (int, optional): Scroll units (positive up, negative down).
            *   `wait_time` (float, optional, default 1.0s): Wait duration in seconds.
            *   `comment` (str, optional): Non-functional comment for readability.
*   **Returns** (object/dict):
    *   `status` (str): "success" or "error".
    *   `message` (str): Result message. Error details if any.
    *   `last_executed_command_index` (int, optional): Index of failed command if error.

## V. Critical Operational Notes & Warnings

*   **API Key**: Ensure `GOOGLE_API_KEY` environment variable is set for `mcp_interactive-gui-mcp_generate_text_from_google` and `mcp_interactive-gui-mcp_get_subarea_description`.
*   **`mcp_interactive-gui-mcp_capture_screen` Storage**: Creates a `tmp` folder in `current_working_dir` for screenshots.
*   **`mcp_interactive-gui-mcp_get_subarea_description` Temp Files**: Generates temporary cropped image files, attempts to delete them after processing.
*   **Coordinate System & `bounds` Parameter for `mcp_interactive-gui-mcp_get_subarea_description` (CRITICAL WARNING)**:
    *   When `bounds` are provided to `mcp_interactive-gui-mcp_get_subarea_description`, the underlying AI model sees **only the cropped sub-image**. It is unaware of the original image dimensions or the `bounds` values.
    *   Therefore, your `prompt` for `mcp_interactive-gui-mcp_get_subarea_description` **must NOT** reference the `x, y, width, height` from the `bounds` to describe the sub-image itself (e.g., avoid "analyze this sub-image at x,y with width,height w,h"). This will confuse the AI.
    *   **Correct Prompts**: Base prompts solely on the visual content of the sub-image provided (e.g., "In this provided image..."). If asking for coordinates of an element *within* the sub-image, ask for them *relative to the top-left of that sub-image*.
    *   **Coordinate Translation**: Any coordinates returned for elements *within* a sub-area are **relative to the sub-area's top-left corner**. You **must** manually add these to the `bounds.x` and `bounds.y` used for cropping to get true global screen coordinates:
        *   `element_global_x = bounds.x + element_relative_x_in_subimage`
        *   `element_global_y = bounds.y + element_relative_y_in_subimage`
*   **`mcp_interactive-gui-mcp_control_input` Safety (CRITICAL WARNING)**: This tool directly controls mouse and keyboard. Use with extreme caution. Supervise scripts using this tool, especially during testing. Start with non-critical applications.

## VI. Guidelines for AI Assistant as MCP Client (Development & Testing Context)

These guidelines apply when the AI assistant is acting as a client to test, debug, or interact with the MCP server's development.

1.  **Confirm File Paths**: Before creating, moving, or modifying files related to MCP server tools or configuration (e.g., in a `tools/` folder or virtual environment), if the target path (project root, specific library path) is unclear, actively confirm with the user.

2.  **Respect Server Tool Signatures (When Assisting with Server Refactoring)**:
    *   If asked to help refactor server-side tool functions (e.g., moving them to separate files):
        *   Ensure tool functions retain their original `async` nature, full docstrings, and type hints.
        *   Advise that the main server script (e.g., `mcp_server.py`) should register imported `async` functions directly using `mcp.tool()(imported_function_name)` to preserve metadata.
        *   Caution against creating extra `async` wrappers in the main server script for imported tools, as this can obscure original docstrings/parameters and lead to incorrect client-side tool discovery.

3.  **Actual Tool Execution (No Simulation)**:
    *   Do not simulate or assume MCP tool execution. **Must** use the specific provided tool-calling capabilities (e.g., `default_api.mcp_interactive-gui-mcp_capture_screen()`) to interact with the MCP server. Report based on actual call results.

4.  **Return Type Consistency (When Assisting with Server Refactoring)**:
    *   If advising on server-side tool implementation, emphasize that functions registered as tools must return the exact object types expected by the MCP framework and client-side tool definitions.

5.  **Iterative Correction and Validation**:
    *   Be prepared to iterate on any process. If observed tool behavior or UI state does not match expectations, re-evaluate, adjust strategy, and retry, explaining the reasoning.

6.  **Proactive Application of Guidelines**:
    *   Actively review and apply all these guidelines in every interaction, even if not explicitly reminded by the user.

7.  **Strict Adherence to API Documentation & User Guidance (Avoid Speculation)**:
    *   User-provided API documentation, code examples, or explicit technical paths are the **highest priority and primary basis** for problem-solving.
    *   **Do not speculate** on API usage (e.g., guessing async method names or parameters) without explicit documentation.
    *   If documentation is insufficient or API usage is unclear, **must actively request clarification, additional documentation, or specific guidance** from the user, rather than attempting unverified methods.
    *   When the user provides corrections or points out correct API usage, **immediately adopt their guidance**.
    *   If self-searching for API documentation, prioritize the latest stable official versions and note the version referenced if possible.














